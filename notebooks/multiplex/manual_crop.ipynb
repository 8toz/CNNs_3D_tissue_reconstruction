{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from natsort import natsorted\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "os.environ[\"OPENCV_IO_MAX_result_PIXELS\"] = pow(2,40).__str__()\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the size of the H&E is the largest, otherwise the alignment might not work\n",
    "\n",
    "for group in natsorted(os.listdir(\"./data/grouped_multiplex\")):\n",
    "    print(group)\n",
    "    print(\"--------\")\n",
    "    multiplex_subgroup = glob.glob(os.path.join(\"./data/grouped_multiplex/\", group, \"*.tif\"))\n",
    "    he_img = cv2.imread(multiplex_subgroup[2])\n",
    "    max_width = he_img.shape[0]\n",
    "    max_height = he_img.shape[1]\n",
    "    if len(multiplex_subgroup) != 5:\n",
    "            print(\"Missing multiplex files in this group, check image size manually\")\n",
    "    else:\n",
    "        for _path in multiplex_subgroup:\n",
    "            img = cv2.imread(_path)\n",
    "            width = img.shape[0] \n",
    "            height = img.shape[1]\n",
    "            \n",
    "            if width > max_width or height > max_height:\n",
    "                print(_path,  \"Warning\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropping_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"307.tif\"\n",
    "image_path = os.path.join('./data/grouped_multiplex/section_10', file_name)\n",
    "\n",
    "#crop_values = (None, None, None, None)\n",
    "crop_values = (1750, 3750, 450, 2550)\n",
    "cropping_dict[file_name] = crop_values\n",
    "image = cv2.imread(image_path)\n",
    "transformed_img = cropped_image = image[crop_values[0]:crop_values[1], crop_values[2]:crop_values[3]]\n",
    "print(transformed_img.shape)\n",
    "plt.imshow(transformed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(image_path, transformed_img)\n",
    "image = cv2.imread(image_path)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./data/cropping_dict.json', 'w') as f:\n",
    "    json.dump(cropping_dict, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
