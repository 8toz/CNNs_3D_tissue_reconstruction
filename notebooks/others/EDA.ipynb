{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO IMAGES MOVED TO MISC FIX RELATIVE PATHS THAT DONT WORK "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the dataset given for the 3D Breast Reconstruction Master Thesis Project\n",
    "\n",
    "The code can be found at https://github.com/8toz/3d_breast_reconstruction/blob/main/EDA.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from scipy import ndimage\n",
    "from tifffile import TiffFile\n",
    "from shapely.affinity import scale, translate\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "\n",
    "## Fine tune opencv to work with large images\n",
    "os.environ[\"OPENCV_IO_MAX_IMAGE_PIXELS\"] = pow(2,40).__str__()\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEOJSON_PATH = './data/GeoJson/H21-066.4_HE332_033_Scan1.qptiff - resolution #1.geojson'\n",
    "GEOJSON_PATH_2 = './data/GeoJson/H21-066.4_HE332_225_Scan1#1.geojson'\n",
    "IMAGE_PATH = './data/66-4/H21-066.4_HE332_033/processed_4/H21-066.4_HE332_033_Scan1.tif'\n",
    "LABEL_DICT = {'epithelium':1, 'blood_vessels':2, 'stroma':3, 'adipocytes':4}\n",
    "COLOR_DICT = {'epithelium':np.array([255, 0, 0]), \n",
    "              'blood_vessels':np.array([0, 0, 255]), \n",
    "              'stroma':np.array([0, 255, 0]) , \n",
    "              'adipocytes':np.array([255, 255, 0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files_with_extension(directory, extension):\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(extension):\n",
    "                file_paths.append(os.path.join(root, file))\n",
    "    return file_paths\n",
    "\n",
    "def get_max_size(path):\n",
    "    with TiffFile(path) as tif:\n",
    "        height, width = -1, -1\n",
    "        for i, page in enumerate(tif.pages):\n",
    "            pages = str(tif.pages[i])\n",
    "            img_height, img_width = get_resolution(pages)\n",
    "            height = img_height if height < img_height else height\n",
    "            width = img_width if width < img_width else width\n",
    "    \n",
    "    return height, width\n",
    "\n",
    "def get_resolution(input_str):\n",
    "    input_str = str(input_str) # Make sure is str format\n",
    "    size = input_str.split(\" \")[4]\n",
    "    height, width = size.split(\"x\")[0], size.split(\"x\")[1]\n",
    "    return int(height), int(width)\n",
    "\n",
    "def get_histograms(ROOT):\n",
    "    hist_b = np.zeros((256,1))\n",
    "    hist_g = np.zeros((256,1))\n",
    "    hist_r = np.zeros((256,1))\n",
    "\n",
    "    hist_gray = np.zeros((256,1))\n",
    "\n",
    "    images_list = os.listdir(ROOT)\n",
    "    n_images = len(images_list)\n",
    "\n",
    "    for img_path in os.listdir(ROOT):\n",
    "        #RGB\n",
    "        image = cv2.imread(os.path.join(ROOT, img_path))\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        hist_b += cv2.calcHist([image_rgb], [0], None, [256], [0, 256])\n",
    "        hist_g += cv2.calcHist([image_rgb], [1], None, [256], [0, 256])\n",
    "        hist_r += cv2.calcHist([image_rgb], [2], None, [256], [0, 256])\n",
    "        #BGR\n",
    "        image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        hist_gray += cv2.calcHist([image_gray], [0], None, [256], [0, 256])\n",
    "\n",
    "    combined_hist_b = hist_b/n_images\n",
    "    combined_hist_g = hist_g/n_images \n",
    "    combined_hist_r = hist_r/n_images\n",
    "\n",
    "    combined_hist_gray = hist_gray/n_images\n",
    "\n",
    "    # Normalize the histograms\n",
    "    hist_b_normalized = combined_hist_b / np.sum(combined_hist_b)\n",
    "    hist_g_normalized = combined_hist_g / np.sum(combined_hist_g)\n",
    "    hist_r_normalized = combined_hist_r / np.sum(combined_hist_r)\n",
    "    hist_bw_normalized =  combined_hist_gray/np.sum(combined_hist_gray)\n",
    "\n",
    "    return hist_b_normalized, hist_g_normalized, hist_r_normalized, hist_bw_normalized\n",
    "\n",
    "def scale_geometry(geom, scaling_factor):\n",
    "    return scale(geom, xfact=scaling_factor, yfact=scaling_factor, zfact=1, origin=(0, 0))\n",
    "\n",
    "def load_labels(path, downscaling_factor=None):\n",
    "    to_drop = [\"id\", \"objectType\", \"classification\", \"object_type\", \"isLocked\"]\n",
    "    labels_df = gpd.read_file(path)\n",
    "    names = list(labels_df[\"classification\"][0].keys())\n",
    "    labels_df[\"label\"] = labels_df[\"classification\"].apply(lambda x: x[names[0]])\n",
    "    #labels_df[\"color\"] = labels_df[\"classification\"].apply(lambda x: rgb_to_hex(x[encoded_names[1]]))\n",
    "    labels_df = labels_df.drop(columns=to_drop, errors=\"ignore\")\n",
    "    # Scale the coordinates\n",
    "    if downscaling_factor is not None:\n",
    "        labels_df['geometry'] = labels_df['geometry'].apply(lambda row: scale_geometry(row, (1/downscaling_factor)))\n",
    "\n",
    "    # Encode and clean labels based in dict\n",
    "    labels_df[\"label\"] = labels_df[\"label\"].str.lower()\n",
    "    labels_df[\"label\"] = labels_df[\"label\"].str.replace(\" \", \"_\")\n",
    "    # Fix because labels are not consistent in the data\n",
    "    labels_df['label'] = labels_df['label'].apply(lambda x: 'blood_vessels' if 'blood_vessel' in x else x)\n",
    "    labels_df['label'] = labels_df['label'].apply(lambda x: 'adipocytes' if 'fat' in x else x)\n",
    "\n",
    "    labels_df[\"encoded_label\"] = labels_df[\"label\"].apply(lambda x: LABEL_DICT[x])\n",
    "\n",
    "    return labels_df\n",
    "\n",
    "def load_image(img_path):\n",
    "    image = cv2.imread(img_path)\n",
    "    return image\n",
    "\n",
    "def get_tif_paths(path, resolution):\n",
    "    path_list = []\n",
    "    for paths in os.walk(path, topdown=False):\n",
    "        root = paths[0]\n",
    "        if \"processed_\"+str(resolution) in root:\n",
    "            for file in paths[-1]:\n",
    "                path_list.append(os.path.join(root, file))\n",
    "    return path_list\n",
    "    \n",
    "def get_labelled_files(labels_path, file_list):\n",
    "    # Returns a tuple with image and labels associated with it\n",
    "    file_label_list = []\n",
    "    if len(file_list) == 0:\n",
    "        raise(\"Create preprocessed files for this resolution\")\n",
    "    \n",
    "    labelled_files = os.listdir(labels_path)\n",
    "    for file in file_list:\n",
    "        file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "        for labelled_file in labelled_files:\n",
    "            if file_name in labelled_file:\n",
    "                file_label_list.append((file, os.path.join(labels_path, labelled_file)))\n",
    "\n",
    "    return file_label_list\n",
    "\n",
    "def get_label_index(path):\n",
    "    if \"epithelium\" in path:\n",
    "        return LABEL_DICT[\"epithelium\"]\n",
    "    elif \"blood_vessels\" in path:\n",
    "        return LABEL_DICT[\"blood_vessels\"]\n",
    "    elif \"stroma\" in path:\n",
    "        return LABEL_DICT[\"stroma\"]\n",
    "    else:\n",
    "        return LABEL_DICT[\"adipocytes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EDA will have 2 sections:\n",
    "- Exploring the image features\n",
    "- Exploring the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_list, width_list = [], []\n",
    "for file in find_files_with_extension(\"./data/66-4\", \".qptiff\"):\n",
    "    height, width = get_max_size(file)\n",
    "    height_list.append(height)\n",
    "    width_list.append(width)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 3))\n",
    "\n",
    "sns.histplot(height_list, ax=ax1, color='blue')\n",
    "sns.histplot(width_list, ax=ax2, color='orange')\n",
    "\n",
    "mean_height = sum(height_list) / len(height_list)\n",
    "mean_width = sum(width_list) / len(width_list)\n",
    "ax1.axvline(mean_height, color='red', linestyle='--', label=f'Mean Height: {mean_height:.0f} pixels.')\n",
    "ax2.axvline(mean_width, color='red', linestyle='--', label=f'Mean Width: {mean_width:.0f} pixels.')\n",
    "ax1.set_title('Height Distribution')\n",
    "ax1.set_xlabel('Height')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax2.set_title('Width Distribution')\n",
    "ax2.set_xlabel('Width')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apreciate there is an image whose size is significantly different for the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"./data/66-4/processed_32/H21-066.4_HE332_237_Scan1.tif\")\n",
    "cropped_image = image[2500:5000,500:2380]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 3))\n",
    "ax1.set_title(\"Outlier\")\n",
    "ax1.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "ax2.set_title(\"Fixed outlier\")\n",
    "ax2.imshow(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB))\n",
    "ax1.axis(\"off\")\n",
    "ax2.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets explore the histograms. We can either work on RGB format which would generate 3 histograms (one per channel) and B&W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"./data/66-4/processed_32\"\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(8, 4))\n",
    "\n",
    "ROOT = \"./data/66-4/processed_32\"\n",
    "\n",
    "hist_b_normalized, hist_g_normalized, hist_r_normalized, hist_bw_normalized = get_histograms(ROOT)\n",
    "\n",
    "# Plot the normalized histograms\n",
    "ax1.set_xlim(0, 256)\n",
    "ax1.plot(hist_b_normalized, color='blue')\n",
    "ax1.plot(hist_g_normalized, color='green')\n",
    "ax1.plot(hist_r_normalized, color='red')\n",
    "ax1.set_xlabel('Pixel Value')\n",
    "ax1.set_ylabel('Normalized Frequency')\n",
    "ax1.set_title('Normalized Histogram of RGB Image')\n",
    "ax1.legend(['Blue Channel', 'Green Channel', 'Red Channel'])\n",
    "\n",
    "# Plot the normalized histogram for the B&W image\n",
    "ax2.set_xlim(0, 256)\n",
    "ax2.plot(hist_bw_normalized, color='black')\n",
    "ax2.set_xlabel('Pixel Value')\n",
    "ax2.set_ylabel('Normalized Frequency')\n",
    "ax2.set_title('Normalized Histogram of B&W Image')\n",
    "\n",
    "\n",
    "# Plot the normalized histograms\n",
    "ax3.set_xlim(200, 256)\n",
    "ax3.plot(hist_b_normalized, color='blue')\n",
    "ax3.plot(hist_g_normalized, color='green')\n",
    "ax3.plot(hist_r_normalized, color='red')\n",
    "ax3.set_xlabel('Pixel Value')\n",
    "ax3.set_ylabel('Normalized Frequency')\n",
    "ax3.set_title('Zoomed Histogram of RGB Image')\n",
    "ax3.legend(['Blue Channel', 'Green Channel', 'Red Channel'])\n",
    "\n",
    "# Plot the normalized histogram for the B&W image\n",
    "ax4.set_xlim(200, 256)\n",
    "ax4.plot(hist_bw_normalized, color='black')\n",
    "ax4.set_xlabel('Pixel Value')\n",
    "ax4.set_ylabel('Normalized Frequency')\n",
    "ax4.set_title('Zoomed Histogram of B&W Image')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the Kmeans algorithm with 6 centroids we can see that, on average, the H&E images have a color distribution higly skewed around purple tones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"./data/66-4/processed_32/H21-066.4_HE332_001_Scan1.tif\")\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "pixels = image_rgb.reshape((-1, 3))\n",
    "num_colors = 6\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
    "_, labels, centers = cv2.kmeans(pixels.astype(np.float32), num_colors, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "centers = np.uint8(centers)\n",
    "reduced_image = centers[labels.flatten()]\n",
    "palette = centers.reshape(1, -1, 3)\n",
    "palette = cv2.cvtColor(palette, cv2.COLOR_BGR2RGB)\n",
    "rotated_img = ndimage.rotate(palette, 90)\n",
    "reduced_image = reduced_image.reshape(image_rgb.shape)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5))\n",
    "ax1.imshow(reduced_image)\n",
    "ax2.imshow(rotated_img, cmap=plt.cm.gray)\n",
    "plt.title('Color Palette')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will discuss the elements of the H&E images and for that we are going to look into the labels. They are stored in a geojson format, where geopandas can come in handy.\n",
    "At this point we have 6 labelled images corresponding to slides:\n",
    "- 033\n",
    "- 089\n",
    "- 105\n",
    "- 137\n",
    "- 225\n",
    "- 285\n",
    "\n",
    "Lets have a look one of them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resolution = \"32\"\n",
    "image = load_image('./data/66-4/processed_'+resolution+'/H21-066.4_HE332_033_Scan1.tif')\n",
    "labels_df  = load_labels('./data/GeoJson/H21-066.4_HE332_033_Scan1.qptiff - resolution #1.geojson', int(resolution))\n",
    "\n",
    "class_colors = {'epithelium':\"#ff0000\", \n",
    "              'blood_vessels':\"#0000ff\", \n",
    "              'stroma':\"#00ff00\", \n",
    "              'adipocytes':\"#9000ff\"}\n",
    "\n",
    "# Plot each geometry with a color depending on the class\n",
    "legend_handles = []\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "for class_label, color in class_colors.items():\n",
    "    subset = labels_df[labels_df['label'] == class_label]\n",
    "    subset.plot(ax=ax, color=color, label=class_label)\n",
    "    legend_handles.append(mpatches.Patch(color=color, label=class_label))\n",
    "\n",
    "plt.legend(handles=legend_handles)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the labels closely and see how they look like. They are also some other elements like air bubbles and dust worth mentioning as they represent a source of noise in the samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE for saving the cropped images \n",
    "\n",
    "# resolution = \"original\"\n",
    "\n",
    "# image = load_image('./data/66-4/H21-066.4_HE332_033/processed_'+resolution+'/H21-066.4_HE332_033_Scan1.tif')\n",
    "# labels_df  = load_labels('./data/GeoJson/H21-066.4_HE332_033_Scan1.qptiff - resolution #1.geojson')\n",
    "# label = \"adipocytes\"\n",
    "\n",
    "# polygon_gdf = labels_df[labels_df[\"label\"]==label][\"geometry\"].iloc[0]\n",
    "# # Get the bounding box of the polygon\n",
    "# min_x, min_y, max_x, max_y = np.floor(polygon_gdf.bounds).astype(\"int64\")\n",
    "# crop = image[min_y:max_y, min_x:max_x]\n",
    "\n",
    "# translate_x = ((max_x-min_x) - polygon_gdf.bounds[0] - polygon_gdf.bounds[2]) / 2\n",
    "# translate_y = ((max_y-min_y) - polygon_gdf.bounds[1] - polygon_gdf.bounds[3]) / 2\n",
    "# translated_polygon = translate(polygon_gdf, xoff=translate_x, yoff=translate_y)\n",
    "\n",
    "# contour = np.array([[int(x), int(y)] for x, y in translated_polygon.exterior.coords])\n",
    "\n",
    "# mask = np.zeros_like(crop)*255\n",
    "# cv2.fillPoly(mask, pts=[contour], color=(255,255,255))\n",
    "# masked_img = cv2.bitwise_and(crop, mask)\n",
    "\n",
    "# cv2.imwrite((label+\".png\"), masked_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2, ax5), (ax3, ax4, ax6)) = plt.subplots(2, 3, figsize=(10, 6))\n",
    "img = load_image(\"./data/66-4/processed_32/H21-066.4_HE332_001_Scan1.tif\")\n",
    "adipocytes = load_image('./misc/adipocytes.png')\n",
    "blood_vessels = load_image(\"./misc/epithelium.png\")\n",
    "stroma = load_image(\"./misc/stroma.png\")\n",
    "epithelium = load_image(\"./misc/blood_vessels.png\")\n",
    "ax1.set_title(\"Adipocytes\", y=-0.2)\n",
    "ax1.imshow(adipocytes)\n",
    "ax1.axis('off')\n",
    "ax2.set_title(\"Blood Vessels\", y=-0.2)\n",
    "ax2.imshow(blood_vessels)\n",
    "ax2.axis('off')\n",
    "ax3.set_title(\"Stroma\", y=-0.15)\n",
    "ax3.imshow(stroma)\n",
    "ax3.axis('off')\n",
    "ax4.set_title(\"Epithelium\", y=-0.2)\n",
    "ax4.imshow(epithelium)\n",
    "ax4.axis('off')\n",
    "ax5.imshow(img[1450:1800, 1200:1600])\n",
    "ax5.set_title(\"Air Bubble\", y=-0.15)\n",
    "ax5.axis('off')\n",
    "ax6.imshow(img[550:700, 50:200])\n",
    "ax6.set_title(\"Dark Hole\", y=-0.15)\n",
    "ax6.axis('off')\n",
    "plt.savefig(\"labels_and_artifacts.png\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some labels are significantly different in size. This will pose a problem in the future as we will we have to deal with class imbalance and overfitting. As we will work with some downsized resolutions the large labels will look the same while others might lose a lot of information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epithelium Downsizing Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store images\n",
    "images = []\n",
    "\n",
    "# Loop through files in the directory\n",
    "for file in os.listdir(\"./misc\"):\n",
    "    if \"smallSeg\" in file:\n",
    "        image = cv2.imread(os.path.join(\"./misc\", file))\n",
    "        # Resize the image to a common size (e.g., 200x200)\n",
    "        resized_image = cv2.resize(image, (1000, 1000))  # Adjust dimensions as needed\n",
    "        # Append the resized image to the list\n",
    "        images.append(resized_image)\n",
    "\n",
    "# Concatenate the images horizontally\n",
    "concatenated_images = cv2.hconcat(images)\n",
    "\n",
    "plt.figure(figsize=(15, 5)) \n",
    "# Show each image with a title\n",
    "list = [\"Original\", \"x2\", \"x4\", \"x8\"]\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    plt.subplot(1, len(images), i + 1)\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(list[i], y=-0.12, fontsize=18)  # Add a title to the image\n",
    "    plt.axis('off')\n",
    "# Show the concatenated image\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adypocytes downsample example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store images\n",
    "images = []\n",
    "\n",
    "# Loop through files in the directory\n",
    "for file in os.listdir():\n",
    "    if \"bigSeg\" in file:\n",
    "        image = cv2.imread(file)\n",
    "        # Resize the image to a common size (e.g., 200x200)\n",
    "        resized_image = cv2.resize(image, (1000, 1000))  # Adjust dimensions as needed\n",
    "        # Append the resized image to the list\n",
    "        images.append(resized_image)\n",
    "\n",
    "# Concatenate the images horizontally\n",
    "concatenated_images = cv2.hconcat(images)\n",
    "\n",
    "plt.figure(figsize=(15, 5)) \n",
    "# Show each image with a title\n",
    "list = [\"Original\", \"x2\", \"x4\", \"x8\"]\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    plt.subplot(1, len(images), i + 1)\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(list[i])  # Add a title to the image\n",
    "    plt.axis('off')\n",
    "# Show the concatenated image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To wrap up lets look at the pixel distribution per label. For that we will extract all the labels for the 6 images in a dataframe and will count the pixels different than zero, which correspond to background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(columns=['geometry', 'label', 'encoded_label'])\n",
    "for f in os.listdir(\"./data/GeoJson\"):\n",
    "    df = load_labels(os.path.join(\"./data/GeoJson\",f))\n",
    "    result_df = pd.concat([result_df, df], axis=0)\n",
    "\n",
    "# Plot the barplot with seaborn\n",
    "sns.countplot(y=result_df[\"label\"], palette=\"viridis\")\n",
    "\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Label\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Distribution of class labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_list = []\n",
    "rgb_list = []\n",
    "for f in tqdm_notebook(image_files):\n",
    "    \n",
    "    image = cv2.imread(f)\n",
    "    rgb_list.append(image)\n",
    "\n",
    "    # Convert the color image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Threshold the grayscale image to get a binary mask of non-black pixels\n",
    "    _, mask = cv2.threshold(gray_image, 1, 255, cv2.THRESH_BINARY)\n",
    "    # Convert the binary mask to a matrix with labels\n",
    "    labels_matrix = np.where(mask > 0, get_label_index(f), 0)\n",
    "    mask_list.append(labels_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the global image matrix\n",
    "matrix_size = 1028\n",
    "global_image = np.zeros((matrix_size, matrix_size))\n",
    "global_rgb = np.zeros((matrix_size, matrix_size, 3), dtype=np.uint8)\n",
    "\n",
    "\n",
    "check = 0\n",
    "x_mark, y_mark = 0, 0\n",
    "\n",
    "# Iterate over each mask\n",
    "for i, labels_matrix in enumerate(mask_list):\n",
    "    #print(i)\n",
    "    #x_mark, y_mark = 0, 0\n",
    "    if check == 1:\n",
    "        break\n",
    "    \n",
    "    # Iterate over rows of the global image\n",
    "    while x_mark + labels_matrix.shape[0] < global_image.shape[0]:\n",
    "        # Check if the mask fits horizontally without going out of bounds\n",
    "        if y_mark + labels_matrix.shape[1] > global_image.shape[1]:\n",
    "            # Move to the next row\n",
    "            x_mark += 1\n",
    "            y_mark = 0\n",
    "            if x_mark == 1028 and y_mark == 1028:\n",
    "                check = 1\n",
    "                break\n",
    "            continue\n",
    "        \n",
    "        # Check if the current position in the global image is empty\n",
    "        if np.all(global_image[x_mark:x_mark+labels_matrix.shape[0], y_mark:y_mark+labels_matrix.shape[1]] == 0):\n",
    "            # Place the mask into the global image\n",
    "            global_image[x_mark:x_mark+labels_matrix.shape[0], y_mark:y_mark+labels_matrix.shape[1]] += labels_matrix\n",
    "            global_rgb[x_mark:x_mark+labels_matrix.shape[0], y_mark:y_mark+labels_matrix.shape[1]] += rgb_list[i]\n",
    "            break\n",
    "        \n",
    "        # Move to the next column\n",
    "        y_mark += 1\n",
    "    \n",
    "unique_values, counts = np.unique(global_image, return_counts=True)\n",
    "\n",
    "# Create a dictionary to store the counts for each value\n",
    "count_dict = dict(zip(unique_values, counts))\n",
    "key_replacements = {0.0:\"background\", 1.0:'epithelium', 2.0:'blood_vessels', 3.0:'stroma', 4.0:'adipocytes'}\n",
    "new_dict = {key_replacements.get(k, k): v for k, v in count_dict.items()}\n",
    "\n",
    "df = pd.DataFrame(list(new_dict.items()), columns=['Label', 'Count'])\n",
    "total_pixels = sum(df[\"Count\"])\n",
    "df[\"percentage\"] = np.round(df[\"Count\"]/total_pixels,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "im1 = axes[0, 0].imshow(global_image, cmap='viridis')\n",
    "fig.colorbar(im1, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Grayscale Image')\n",
    "\n",
    "axes[0, 1].imshow(global_rgb)\n",
    "axes[0, 1].set_title('RGB Image')\n",
    "\n",
    "# Bar plot using Seaborn\n",
    "sns.barplot(x='Label', y='Count', data=df, ax=axes[1, 0], color='skyblue')\n",
    "axes[1, 0].set_title('Distribution of pixels per label')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "\n",
    "# Sample data\n",
    "label = df[\"Label\"].values\n",
    "colors = ['gray', '#ff0000', '#0000ff', '#00ff00', \"#9000ff\"]\n",
    "sizes = df[\"percentage\"].values\n",
    "\n",
    "\n",
    "axes[1, 1].pie(sizes, labels=label, colors=colors, autopct='%1.1f%%', startangle=90, pctdistance=1.2, textprops={'size': 8},wedgeprops=dict(edgecolor='black'), labeldistance=None, radius=1)\n",
    "axes[1, 1].set_title('Distribution of pixels per label')\n",
    "axes[1, 1].legend(labels, loc=\"upper left\", bbox_to_anchor=(-0.5,1))\n",
    "\n",
    "\n",
    "plt.title('% of pixels per label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this EDA we can conclude the following:\n",
    "- We need to account for outliers while reading the image and crop only the regions of interest.\n",
    "- The codensed distribution of colors around pinks and blues might make the classification task challenging in terms of differences in pixel intensities.\n",
    "- The label imbalance needs to be handled with data augmentation or additional labeling effort from the side of the researchers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
